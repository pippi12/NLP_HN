{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レビューを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_model_name(filepath):\n",
    "    filename = os.path.split(filepath)[1]\n",
    "    idx1 = filename.find('_')\n",
    "    idx2 = filename.find('.')\n",
    "    model_name = filename[idx1+1:idx2]\n",
    "    return model_name\n",
    "\n",
    "files = glob.glob(r'./reviews/reviews_*.csv')\n",
    "list_modelreview = []\n",
    "for file in files:\n",
    "    model_name = get_model_name(file)\n",
    "    df = pd.read_csv(file, encoding='utf_8_sig')\n",
    "    list_modelreview.append([model_name, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部のレビューを処理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "titles = []\n",
    "contents = []\n",
    "for m in list_modelreview:\n",
    "    df = m[1]\n",
    "    # タイトルとレビューを連結\n",
    "    ttls = df['title'].fillna('').tolist()\n",
    "    cnts = df['review'].fillna('').tolist()\n",
    "    revs = (df['title'].fillna('') + '. ' + df['review'].fillna('')).tolist()\n",
    "    reviews.extend(revs)\n",
    "    titles.extend(ttls)\n",
    "    contents.extend(cnts)\n",
    "\n",
    "print('総レビュー数: ' + str(len(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特定モデルのレビューを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'WHE6060SB'\n",
    "for m in list_modelreview:\n",
    "    if m[0] == model_name:\n",
    "        df_target = m[1]\n",
    "titles_target = df_target['title'].fillna('').tolist()\n",
    "contents_target = df_target['review'].fillna('').tolist()\n",
    "reviews_target = (df_target['title'].fillna('') + '. ' + df_target['review'].fillna('')).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 短縮表現 ===\n",
    "import re\n",
    "shortened = {\n",
    "    '\\'m': ' am',\n",
    "    '\\'re': ' are',\n",
    "    'don\\'t': 'do not',\n",
    "    'doesn\\'t': 'does not',\n",
    "    'didn\\'t': 'did not',\n",
    "    'won\\'t': 'will not',\n",
    "    'wanna': 'want to',\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    'hafta': 'have to',\n",
    "    'needa': 'need to',\n",
    "    'outta': 'out of',\n",
    "    'kinda': 'kind of',\n",
    "    'sorta': 'sort of',\n",
    "    'lotta': 'lot of',\n",
    "    'lemme': 'let me',\n",
    "    'gimme': 'give me',\n",
    "    'getcha': 'get you',\n",
    "    'gotcha': 'got you',\n",
    "    'letcha': 'let you',\n",
    "    'betcha': 'bet you',\n",
    "    'shoulda': 'should have',\n",
    "    'coulda': 'could have',\n",
    "    'woulda': 'would have',\n",
    "    'musta': 'must have',\n",
    "    'mighta': 'might have',\n",
    "    'dunno': 'do not know',\n",
    "    'i\\'ll': 'i will',\n",
    "    'it\\'s': 'it is',\n",
    "    'it’s': 'it is'\n",
    "}\n",
    "shortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')\n",
    "# ストップセンテンスを除去\n",
    "stop_sentences = ['[this review was collected as part of a promotion.]']\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "'''\n",
    "レビューに諸々の前処理を実行する．\n",
    "'''\n",
    "def cleaned_sentences(review):\n",
    "    # 小文字化\n",
    "    review_ = review.lower()\n",
    "    # 空白の削除\n",
    "    review_ = review_.rstrip()\n",
    "    # 短縮表現を戻す\n",
    "    review_ = shortened_re.sub(lambda x: shortened[x.group(0)], review_)\n",
    "\n",
    "    # レビューを文区切り\n",
    "    token_review = sent_tokenize(review_)\n",
    "    token_review = [sen.rstrip() for sen in token_review] # 空白の削除\n",
    "    token_review = [s for s in token_review if s not in stop_sentences]\n",
    "\n",
    "    return token_review\n",
    "\n",
    "# ストップワードを除去\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_add = [',','.','!','?','-','(',')',':',';','\\'','..','’','...','&','“','”','fridge','\\'\\'','\\'ve','\\'s','\\'d','....','.....','......']\n",
    "# 語幹へ修正\n",
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "# 句点などを除去\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "'''\n",
    "文を単語に分割し，諸々の前処理を実行する．\n",
    "'''\n",
    "def cleaned_words(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    token_sentence = []\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if (word not in stop_words and # stopwordsを除去する。\n",
    "            word not in stop_words_add and # stopwordsを除去する。\n",
    "            word not in punctuations): # punctuationを除去する。\n",
    "            if tag.startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            \n",
    "            # Normalize sentence\n",
    "            stem_word = lemmatizer.lemmatize(word, pos)\n",
    "            if len(stem_word) > 0:\n",
    "                token_sentence.append(stem_word)\n",
    "    return token_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(articles):\n",
    "    sentences_rev = []\n",
    "    sentences = []\n",
    "    reviews_cleaned = []\n",
    "    for review in articles:\n",
    "        sentences_tmp = cleaned_sentences(review)\n",
    "        sentences_rev.append(sentences_tmp)\n",
    "        sentences.extend(sentences_tmp)\n",
    "        reviews_cleaned.append(' '.join(sentences_tmp))\n",
    "\n",
    "    words_sen = [cleaned_words(s) for s in sentences]\n",
    "    words_rev = [cleaned_words(r) for r in reviews_cleaned]\n",
    "    words = [x for row in words_sen for x in row]\n",
    "\n",
    "    return reviews_cleaned, sentences, sentences_rev, words_rev, words_sen, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_revs = preprocess(reviews)\n",
    "pre_ttls = preprocess(titles)\n",
    "pre_cnts = preprocess(contents)\n",
    "\n",
    "pre_revs_target = preprocess(reviews_target)\n",
    "pre_ttls_target = preprocess(titles_target)\n",
    "pre_cnts_target = preprocess(contents_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloudで可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "def make_wordcloud(ofile, words):\n",
    "    text = ' '.join(words)\n",
    "    wordcloud = WordCloud(background_color='white', width=800, height=800).generate(text)\n",
    "    wordcloud.to_file(ofile)\n",
    "\n",
    "freq_dist = nltk.FreqDist(pre_ttls[5])\n",
    "print(freq_dist.most_common(10))\n",
    "freq_dist = nltk.FreqDist(pre_cnts[5])\n",
    "print(freq_dist.most_common(10))\n",
    "\n",
    "make_wordcloud(r'./out/wordcloud_title.png', pre_ttls[5])\n",
    "make_wordcloud(r'./out/wordcloud_content.png', pre_cnts[5])\n",
    "make_wordcloud(r'./out/wordcloud_review.png', pre_revs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vader\n",
    "https://qiita.com/y_itoh/items/fb24de866ae132d0ec3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "import pandas as pd\n",
    "\n",
    "def make_vader(sentences):\n",
    "    # スコアを取得\n",
    "    result = []\n",
    "    for s in sentences:\n",
    "        score = vader_analyzer.polarity_scores(s)\n",
    "        result.append(score)\n",
    "\n",
    "    # 辞書型からデータフレームに変換\n",
    "    i = 0\n",
    "    df = pd.DataFrame()\n",
    "    for i, s in enumerate(sentences):\n",
    "        x = pd.DataFrame.from_dict(result[i], orient='index').T\n",
    "        df = pd.concat([df,x], ignore_index=True)\n",
    "    df.index = sentences\n",
    "\n",
    "    return df\n",
    "\n",
    "df_vader_ttls = make_vader(pre_ttls[0])\n",
    "df_vader_cnts = make_vader(pre_cnts[0])\n",
    "df_vader_revs = make_vader(pre_revs[0])\n",
    "\n",
    "with pd.ExcelWriter(r'./out/sentimental_analysis.xlsx') as writer:\n",
    "    df_vader_ttls.to_excel(writer, sheet_name='titles')\n",
    "    df_vader_cnts.to_excel(writer, sheet_name='contents')\n",
    "    df_vader_revs.to_excel(writer, sheet_name='reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "https://www.dskomei.com/entry/2018/04/11/001944\n",
    "\n",
    "https://qiita.com/kenta1984/items/b08d5caeed6ed9c8abf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "def lda_training(train_texts, ntopic):\n",
    "    dictionary = Dictionary(train_texts)\n",
    "    dictionary.filter_extremes(no_below=3, no_above=0.4, keep_n=100000, keep_tokens=None)\n",
    "    dictionary.save_as_text('./lda/dict.txt')\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "    corpora.MmCorpus.serialize('./lda/cop.mm', corpus)\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "    print(lda.show_topics())\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "def lda_test(test_texts, sentences, ntopic, ofile):\n",
    "    # 読み込み\n",
    "    dictionary = corpora.Dictionary.load_from_text(r'./lda/dict.txt')\n",
    "    corpus = corpora.MmCorpus(r'./lda/cop.mm')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "\n",
    "    score_by_topic = defaultdict(int)\n",
    "    test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
    "\n",
    "    # クラスタリング結果を出力\n",
    "    scores = []\n",
    "    for unseen_doc, raw_train_text in zip(test_corpus, sentences):\n",
    "        score_sen = []\n",
    "        score_sen.append(raw_train_text)\n",
    "        for topic, score in lda[unseen_doc]:\n",
    "            score_by_topic[int(topic)] = float(score)\n",
    "        for i in range(ntopic):\n",
    "            score_sen.append(score_by_topic[i])\n",
    "        scores.append(score_sen)\n",
    "\n",
    "    col = ['sentence']\n",
    "    col.extend([str(i+1) for i in range(ntopic)])\n",
    "    df_fdist = pd.DataFrame(scores, columns=col)\n",
    "    ofile_path = r'./lda/class_' + ofile + '.csv'\n",
    "    df_fdist.to_csv(ofile_path, encoding='utf_8_sig')\n",
    "\n",
    "    return test_corpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "def lda_visualize(test_corpus, ofile):\n",
    "    dictionary = corpora.Dictionary.load_from_text(r'./lda/dict.txt')\n",
    "    corpus = corpora.MmCorpus(r'./lda/cop.mm')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "\n",
    "    lda_viz = gensimvis.prepare(lda, test_corpus, dictionary)\n",
    "    pyLDAvis.save_html(lda_viz, r'./lda/pyldavis_' + ofile +'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopic = 3\n",
    "lda_training(pre_revs[3],ntopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopic = 3\n",
    "test_corpus = lda_test(pre_revs_target[3], pre_revs_target[0], ntopic, 'rev')\n",
    "lda_visualize(test_corpus, 'rev')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1260ba601d628ec0893f131d16bda34701c0ac703adb4cb400852db627c510d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レビューを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_model_name(filepath):\n",
    "    filename = os.path.split(filepath)[1]\n",
    "    idx1 = filename.find('_')\n",
    "    idx2 = filename.find('.')\n",
    "    model_name = filename[idx1+1:idx2]\n",
    "    return model_name\n",
    "\n",
    "files = glob.glob('./reviews/reviews_*.csv')\n",
    "list_modelreview = []\n",
    "for file in files:\n",
    "    model_name = get_model_name(file)\n",
    "    df = pd.read_csv(file, encoding='utf_8_sig')\n",
    "    list_modelreview.append([model_name, df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全部のレビューを処理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for m in list_modelreview:\n",
    "    df = m[1]\n",
    "    # タイトルとレビューを連結\n",
    "    r = (df['title'].fillna('') + '. ' + df['review'].fillna('')).tolist()\n",
    "    #r = df['review'].fillna('').tolist()\n",
    "    reviews.extend(r)\n",
    "print('総レビュー数: ' + str(len(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 短縮表現 ===\n",
    "import re\n",
    "shortened = {\n",
    "    '\\'m': ' am',\n",
    "    '\\'re': ' are',\n",
    "    'don\\'t': 'do not',\n",
    "    'doesn\\'t': 'does not',\n",
    "    'didn\\'t': 'did not',\n",
    "    'won\\'t': 'will not',\n",
    "    'wanna': 'want to',\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    'hafta': 'have to',\n",
    "    'needa': 'need to',\n",
    "    'outta': 'out of',\n",
    "    'kinda': 'kind of',\n",
    "    'sorta': 'sort of',\n",
    "    'lotta': 'lot of',\n",
    "    'lemme': 'let me',\n",
    "    'gimme': 'give me',\n",
    "    'getcha': 'get you',\n",
    "    'gotcha': 'got you',\n",
    "    'letcha': 'let you',\n",
    "    'betcha': 'bet you',\n",
    "    'shoulda': 'should have',\n",
    "    'coulda': 'could have',\n",
    "    'woulda': 'would have',\n",
    "    'musta': 'must have',\n",
    "    'mighta': 'might have',\n",
    "    'dunno': 'do not know',\n",
    "    'i\\'ll': 'i will',\n",
    "    'it\\'s': 'it is',\n",
    "    'it’s': 'it is'\n",
    "}\n",
    "shortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')\n",
    "\n",
    "# ストップワードを除去\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_add = [',','.','!','?','-','(',')',':',';','\\'','..','’','...','&','“','”','fridge','\\'\\'','\\'ve','\\'s','\\'d','....','.....','......']\n",
    "# 語幹へ修正\n",
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "# 句点などを除去\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def cleaned_words(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words_clean = []\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if (word not in stop_words and # stopwordsを除去する。\n",
    "            word not in stop_words_add and # stopwordsを除去する。\n",
    "            word not in punctuations): # punctuationを除去する。\n",
    "            if tag.startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            \n",
    "            # Normalize sentence\n",
    "            stem_word = lemmatizer.lemmatize(word, pos)\n",
    "            if len(stem_word) > 0:\n",
    "                words_clean.append(stem_word)\n",
    "    return words_clean\n",
    "\n",
    "print(cleaned_words('I am swimming. You are most highest human'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小文字化\n",
    "reviews = [review.lower() for review in reviews]\n",
    "# 空白の削除\n",
    "reviews = [review.rstrip() for review in reviews]\n",
    "# 短縮表現を戻す\n",
    "reviews = [shortened_re.sub(lambda x: shortened[x.group(0)], review) for review in reviews]\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences_rev = []\n",
    "sentences = []\n",
    "words_rev = []\n",
    "\n",
    "# レビューを文区切り\n",
    "stop_sentences = ['[this review was collected as part of a promotion.]']\n",
    "for review in reviews:\n",
    "    token_review = sent_tokenize(review)\n",
    "    token_review = [sen.rstrip() for sen in token_review] # 空白の削除\n",
    "    token_review = [s for s in token_review if s not in stop_sentences]\n",
    "    sentences_rev.append(token_review)\n",
    "    sentences.extend(token_review)\n",
    "\n",
    "sentence_rev2 = []\n",
    "for s in sentences_rev:\n",
    "    sentence_rev2.append(' '.join(s))\n",
    "\n",
    "words_sen = [cleaned_words(sentence) for sentence in sentences]\n",
    "#words_rev = [cleaned_words(review) for review in reviews]\n",
    "words_rev = [cleaned_words(review) for review in sentence_rev2]\n",
    "words = [x for row in words_sen for x in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloudで可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(words)\n",
    "print(freq_dist.most_common(10))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "ofile_wordcloud = './out/wordcloud.png'\n",
    "#font_path\n",
    "text = ' '.join(words)\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=800).generate(text)\n",
    "wordcloud.to_file(ofile_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vader\n",
    "https://qiita.com/y_itoh/items/fb24de866ae132d0ec3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "import pandas as pd\n",
    "\n",
    "# スコアを取得\n",
    "result = []\n",
    "for s in sentences:\n",
    "    score = vader_analyzer.polarity_scores(s)\n",
    "    result.append(score)\n",
    "\n",
    "# 辞書型からデータフレームに変換\n",
    "i = 0\n",
    "df = pd.DataFrame()\n",
    "for i, s in enumerate(sentences):\n",
    "    x = pd.DataFrame.from_dict(result[i], orient='index').T\n",
    "    df = pd.concat([df,x], ignore_index=True)\n",
    "df.index = sentences\n",
    "\n",
    "with pd.ExcelWriter('./out/sentimentalanalysis.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "https://www.dskomei.com/entry/2018/04/11/001944\n",
    "\n",
    "https://qiita.com/kenta1984/items/b08d5caeed6ed9c8abf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "def lda_training(train_texts, ntopic):\n",
    "    dictionary = Dictionary(train_texts)\n",
    "    dictionary.filter_extremes(no_below=3, no_above=0.4, keep_n=100000, keep_tokens=None)\n",
    "    dictionary.save_as_text('./lda/dict.txt')\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "    corpora.MmCorpus.serialize('./lda/cop.mm', corpus)\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "    print(lda.show_topics())\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "def lda_test(test_texts, ntopic):\n",
    "    # 読み込み\n",
    "    dictionary = gensim.corpora.Dictionary.load_from_text('./lda/dict.txt')\n",
    "    corpus = corpora.MmCorpus('./lda/cop.mm')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "\n",
    "    score_by_topic = defaultdict(int)\n",
    "    test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
    "\n",
    "    # クラスタリング結果を出力\n",
    "    scores = []\n",
    "    for unseen_doc, raw_train_text in zip(test_corpus, sentences):\n",
    "        score_sen = []\n",
    "        score_sen.append(raw_train_text)\n",
    "        for topic, score in lda[unseen_doc]:\n",
    "            score_by_topic[int(topic)] = float(score)\n",
    "        for i in range(ntopic):\n",
    "            score_sen.append(score_by_topic[i])\n",
    "        scores.append(score_sen)\n",
    "\n",
    "    col = ['sentence']\n",
    "    col.extend([str(i+1) for i in range(ntopic)])\n",
    "    df_fdist = pd.DataFrame(scores, columns=col)\n",
    "    ofile_path = './lda/topic_classification.csv'\n",
    "    df_fdist.to_csv(ofile_path, encoding='utf_8_sig')\n",
    "\n",
    "    return test_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopic = 3\n",
    "lda_training(words_rev,ntopic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopic = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "lda_viz = gensimvis.prepare(lda, test_corpus, dictionary)\n",
    "pyLDAvis.save_html(lda_viz, './out/pyldavis_output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1260ba601d628ec0893f131d16bda34701c0ac703adb4cb400852db627c510d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 口コミを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_name = 'WHE6060SB'\n",
    "df = pd.read_csv('./reviews/reviews_' + model_name + '.csv', header=0, encoding='utf_8_sig')\n",
    "reviews = (df['title'].fillna('') + '. ' + df['review'].fillna('')).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== 短縮表現 ===\n",
    "import re\n",
    "shortened = {\n",
    "    '\\'m': ' am',\n",
    "    '\\'re': ' are',\n",
    "    'don\\'t': 'do not',\n",
    "    'doesn\\'t': 'does not',\n",
    "    'didn\\'t': 'did not',\n",
    "    'won\\'t': 'will not',\n",
    "    'wanna': 'want to',\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    'hafta': 'have to',\n",
    "    'needa': 'need to',\n",
    "    'outta': 'out of',\n",
    "    'kinda': 'kind of',\n",
    "    'sorta': 'sort of',\n",
    "    'lotta': 'lot of',\n",
    "    'lemme': 'let me',\n",
    "    'gimme': 'give me',\n",
    "    'getcha': 'get you',\n",
    "    'gotcha': 'got you',\n",
    "    'letcha': 'let you',\n",
    "    'betcha': 'bet you',\n",
    "    'shoulda': 'should have',\n",
    "    'coulda': 'could have',\n",
    "    'woulda': 'would have',\n",
    "    'musta': 'must have',\n",
    "    'mighta': 'might have',\n",
    "    'dunno': 'do not know',\n",
    "    'i\\'ll': 'i will',\n",
    "    'it\\'s': 'it is',\n",
    "    'it’s': 'it is'\n",
    "}\n",
    "shortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')\n",
    "\n",
    "# ストップワードを除去\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_add = [',','.','!','?','-','(',')',':',';','\\'','..','’','...','&','“','”','fridge']\n",
    "# 語幹へ修正\n",
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "# 句点などを除去\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def cleaned_words(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#    sentence = shortened_re.sub(lambda x: shortened[x.group(0)], sentence)\n",
    "    words_clean = []\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if (word not in stop_words and # stopwordsを除去する。\n",
    "            word not in stop_words_add and # stopwordsを除去する。\n",
    "            word not in punctuations): # punctuationを除去する。\n",
    "            if tag.startswith(\"NN\"):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            \n",
    "            # Normalize sentence\n",
    "            stem_word = lemmatizer.lemmatize(word, pos)\n",
    "            if len(stem_word) > 0:\n",
    "                words_clean.append(stem_word)\n",
    "    return words_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小文字化\n",
    "reviews = [review.lower() for review in reviews]\n",
    "# 空白の削除\n",
    "reviews = [review.rstrip() for review in reviews]\n",
    "# 短縮表現を戻す\n",
    "reviews = [shortened_re.sub(lambda x: shortened[x.group(0)], review) for review in reviews]\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentences_rev = []\n",
    "sentences = []\n",
    "words_rev = []\n",
    "\n",
    "# レビューを文区切り\n",
    "for review in reviews:\n",
    "    token_review = sent_tokenize(review)\n",
    "    token_review = [sen.rstrip() for sen in token_review] # 空白の削除\n",
    "    sentences_rev.append(token_review)\n",
    "    sentences.extend(token_review)\n",
    "\n",
    "sentence_rev2 = []\n",
    "for s in sentences_rev:\n",
    "    sentence_rev2.append(' '.join(s))\n",
    "\n",
    "words_sen = [cleaned_words(sentence) for sentence in sentences]\n",
    "#words_rev = [cleaned_words(review) for review in reviews]\n",
    "words_rev = [cleaned_words(review) for review in sentence_rev2]\n",
    "words = [x for row in words_sen for x in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloudで可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = nltk.FreqDist(words)\n",
    "print(freq_dist.most_common(10))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "ofile_wordcloud = './data/wordcloud_' + modelname + '.png'\n",
    "#font_path\n",
    "text = ' '.join(words)\n",
    "wordcloud = WordCloud(background_color='white', width=800, height=800).generate(text)\n",
    "wordcloud.to_file(ofile_wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極性分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vader\n",
    "https://qiita.com/y_itoh/items/fb24de866ae132d0ec3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "import pandas as pd\n",
    "\n",
    "# スコアを取得\n",
    "result = []\n",
    "for s in sentences:\n",
    "    score = vader_analyzer.polarity_scores(s)\n",
    "    result.append(score)\n",
    "\n",
    "# 辞書型からデータフレームに変換\n",
    "i = 0\n",
    "df = pd.DataFrame()\n",
    "for i, s in enumerate(sentences):\n",
    "    x = pd.DataFrame.from_dict(result[i], orient='index').T\n",
    "    df = pd.concat([df,x], ignore_index=True)\n",
    "df.index = sentences\n",
    "\n",
    "with pd.ExcelWriter('./data/sentimentalanalysis.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name=modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "https://www.dskomei.com/entry/2018/04/11/001944\n",
    "\n",
    "https://qiita.com/kenta1984/items/b08d5caeed6ed9c8abf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "def lda_test(test_texts, ntopic):\n",
    "    # 読み込み\n",
    "    dictionary = corpora.Dictionary.load_from_text('./lda/dict.txt')\n",
    "    corpus = corpora.MmCorpus('./lda/cop.mm')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "\n",
    "    score_by_topic = defaultdict(int)\n",
    "    test_corpus = [dictionary.doc2bow(text) for text in test_texts]\n",
    "\n",
    "    # クラスタリング結果を出力\n",
    "    scores = []\n",
    "    for unseen_doc, raw_train_text in zip(test_corpus, sentences):\n",
    "        score_sen = []\n",
    "        score_sen.append(raw_train_text)\n",
    "        for topic, score in lda[unseen_doc]:\n",
    "            score_by_topic[int(topic)] = float(score)\n",
    "        for i in range(ntopic):\n",
    "            score_sen.append(score_by_topic[i])\n",
    "        scores.append(score_sen)\n",
    "\n",
    "    col = ['sentence']\n",
    "    col.extend([str(i+1) for i in range(ntopic)])\n",
    "    df_fdist = pd.DataFrame(scores, columns=col)\n",
    "    ofile_path = './lda/topic_classification.csv'\n",
    "    df_fdist.to_csv(ofile_path, encoding='utf_8_sig')\n",
    "\n",
    "    return test_corpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "def lda_visualize(test_corpus):\n",
    "    dictionary = corpora.Dictionary.load_from_text('./lda/dict.txt')\n",
    "    corpus = corpora.MmCorpus('./lda/cop.mm')\n",
    "    lda = LdaModel(corpus=corpus, num_topics=ntopic, id2word=dictionary)\n",
    "\n",
    "    lda_viz = gensimvis.prepare(lda, test_corpus, dictionary)\n",
    "    pyLDAvis.save_html(lda_viz, './lda/pyldavis_output.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopic = 3\n",
    "test_corpus = lda_test(words_rev, ntopic)\n",
    "lda_visualize(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "dictionary = Dictionary(words_sen)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.4, keep_n=100000, keep_tokens=None)\n",
    "corpus = [dictionary.doc2bow(text) for text in words_sen]\n",
    "NUM_TOPICS = 3\n",
    "lda = LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    "print(lda.show_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "score_by_topic = defaultdict(int)\n",
    "test_corpus = [dictionary.doc2bow(text) for text in words_sen]\n",
    "\n",
    "# クラスタリング結果を出力\n",
    "scores = []\n",
    "for unseen_doc, raw_train_text in zip(test_corpus, sentences):\n",
    "    score_sen = []\n",
    "    score_sen.append(raw_train_text)\n",
    "    for topic, score in lda[unseen_doc]:\n",
    "        score_by_topic[int(topic)] = float(score)\n",
    "    for i in range(NUM_TOPICS):\n",
    "        score_sen.append(score_by_topic[i])\n",
    "    scores.append(score_sen)\n",
    "\n",
    "df_fdist = pd.DataFrame(scores, columns=['sentence', '1', '2', '3'])\n",
    "ofile_path = './data/topicclassification_' + modelname + '.csv'\n",
    "df_fdist.to_csv(ofile_path, encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "lda_viz = gensimvis.prepare(lda, test_corpus, dictionary)\n",
    "pyLDAvis.save_html(lda_viz, './pyldavis_output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim  import corpora\n",
    "\n",
    "# 単語の出現回数を格納する変数を定義\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "# 単語の出現回数をカウント\n",
    "for word in words:\n",
    "    frequency[word] += 1\n",
    "\n",
    "# 2回以上出現した単語のみで配列を構築\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(min_df=0.01, max_df=0.5, ngram_range=(1,2))\n",
    "\n",
    "sentences_tfidf = [' '.join(tokens) for tokens in words_rev]\n",
    "cv.fit(sentences_tfidf)\n",
    "cv_counts = cv.transform(sentences_tfidf)\n",
    "\n",
    "transformed_weights = TfidfTransformer().fit_transform(cv_counts)\n",
    "features = {}\n",
    "\n",
    "for feature, weight in zip(cv.get_feature_names(), np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()):\n",
    "    features[feature] = weight\n",
    "\n",
    "sorted_features = [(key, features[key]) for key in sorted(features, key=features.get, reverse=True)]\n",
    "sorted_features[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9610636acfcb408f50cd16af9eb59fe46745b55b93c04ee46e631e3a7de6034"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('3.7.9': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
